[
  {
    "id": "first-post",
    "title": "How I Built This Site",
    "date": "2025-06-23",
    "description": "A breakdown of how I built this project using React and Bootstrap.",
    "thumbnail": "helloworld.png",
    "content": {
      "sections": [
        {
          "type": "paragraph",
          "content": "Building a personal website is always an exciting journey. In this post, I'll walk you through how I created this blog using React and modern web technologies."
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Tech Stack"
        },
        {
          "type": "paragraph",
          "content": "For this project, I chose React as the frontend framework due to its component-based architecture and excellent developer experience. Here's what I used:"
        },
        {
          "type": "list",
          "items": [
            "React 18 for the UI framework",
            "React Router for navigation",
            "Bootstrap for responsive design",
            "CSS3 for custom styling",
            "JSON for data management"
          ]
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Architecture"
        },
        {
          "type": "paragraph",
          "content": "The site follows a clean, component-based architecture where each page is a separate component. Instead of using markdown files, I opted for a JSON-based content management system that provides better performance and easier maintenance."
        },
        {
          "type": "code",
          "language": "javascript",
          "content": "// Example component structure\nfunction BlogPost() {\n  const [blogData, setBlogData] = useState(null);\n  \n  useEffect(() => {\n    // Load blog content from JSON\n    const blog = blogData.find(b => b.id === id);\n    setBlogData(blog);\n  }, [id]);\n  \n  return (\n    <div className=\"blog-content\">\n      <h1>{blogData.title}</h1>\n      {blogData.content.sections.map(section => (\n        <RenderSection key={section.id} section={section} />\n      ))}\n    </div>\n  );\n}"
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Design Philosophy"
        },
        {
          "type": "paragraph",
          "content": "The design focuses on a clean, matte black aesthetic with subtle gradients and smooth animations. The color scheme emphasizes readability while maintaining a modern, professional look."
        },
        {
          "type": "paragraph",
          "content": "Key design principles include:"
        },
        {
          "type": "list",
          "items": [
            "High contrast for accessibility",
            "Smooth transitions and hover effects",
            "Responsive design for all devices",
            "Clean typography with Inter font",
            "Subtle shadows and depth"
          ]
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Performance Optimizations"
        },
        {
          "type": "paragraph",
          "content": "To ensure fast loading times, I implemented several optimizations:"
        },
        {
          "type": "list",
          "items": [
            "Lazy loading for images",
            "Component-based code splitting",
            "Optimized CSS with minimal reflows",
            "Efficient state management",
            "JSON-based content loading"
          ]
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Future Enhancements"
        },
        {
          "type": "paragraph",
          "content": "The site is designed to be easily extensible. Some planned features include:"
        },
        {
          "type": "list",
          "items": [
            "Comment system",
            "Social media integration",
            "Analytics dashboard"
          ]
        },
        {
          "type": "paragraph",
          "content": "Building this website has been a great learning experience. The combination of React's flexibility and modern CSS techniques has resulted in a fast, beautiful, and maintainable codebase."
        }
      ]
    }
  },
  {
    "id": "intro-to-react",
    "title": "Intro to React",
    "date": "2025-06-20",
    "description": "Everything you need to know to start React from scratch.",
    "thumbnail": "default.png",
    "content": {
      "sections": [
        {
          "type": "paragraph",
          "content": "React is a powerful JavaScript library for building user interfaces. In this comprehensive guide, I'll walk you through the fundamentals of React development."
        },
        {
          "type": "heading",
          "level": 2,
          "content": "What is React?"
        },
        {
          "type": "paragraph",
          "content": "React is a declarative, efficient, and flexible JavaScript library for building user interfaces. It lets you compose complex UIs from small and isolated pieces of code called 'components'."
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Key Concepts"
        },
        {
          "type": "list",
          "items": [
            "Components: Reusable UI pieces",
            "Props: Data passed to components",
            "State: Internal component data",
            "JSX: JavaScript syntax extension",
            "Virtual DOM: Efficient rendering"
          ]
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Getting Started"
        },
        {
          "type": "code",
          "language": "bash",
          "content": "npx create-react-app my-app\ncd my-app\nnpm start"
        },
        {
          "type": "paragraph",
          "content": "This will create a new React project with all the necessary dependencies and start the development server."
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Your First Component"
        },
        {
          "type": "code",
          "language": "jsx",
          "content": "function Welcome(props) {\n  return <h1>Hello, {props.name}!</h1>;\n}\n\nfunction App() {\n  return (\n    <div>\n      <Welcome name=\"React Developer\" />\n    </div>\n  );\n}"
        },
        {
          "type": "paragraph",
          "content": "This simple example shows how components work in React. The Welcome component receives props and renders them in JSX."
        },
        {
          "type": "heading",
          "level": 2,
          "content": "State Management"
        },
        {
          "type": "paragraph",
          "content": "State allows components to manage their own data and re-render when that data changes."
        },
        {
          "type": "code",
          "language": "jsx",
          "content": "import { useState } from 'react';\n\nfunction Counter() {\n  const [count, setCount] = useState(0);\n  \n  return (\n    <div>\n      <p>Count: {count}</p>\n      <button onClick={() => setCount(count + 1)}>\n        Increment\n      </button>\n    </div>\n  );\n}"
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Best Practices"
        },
        {
          "type": "list",
          "items": [
            "Keep components small and focused",
            "Use meaningful component names",
            "Extract reusable logic into custom hooks",
            "Optimize performance with React.memo",
            "Follow the single responsibility principle"
          ]
        },
        {
          "type": "paragraph",
          "content": "React is an excellent choice for building modern web applications. Its component-based architecture and rich ecosystem make it a powerful tool for developers."
        }
      ]
    }
  },
  {
    "id": "gdnet-project-overview",
    "title": "GDnet Project Overview",
    "date": "2025-06-28",
    "description": "How i made my minimalistic educational  Deep Learning framework using cupy for GPU acceleration",
    "thumbnail": "deeplearning.png",
    "content": {
      "sections": [
        {
          "type": "heading",
          "content": "Kick off"
        },
        {
          "type": "paragraph",
          "content": "I always wanted to understand how deep learning frameworks like TensorFlow or PyTorch really work under the hood. So I dived into the math and quickly whipped up a basic NumPy-based neural net. At first, I thought, â€œThis is a piece of cake â€” just WÂ·X + B and boom, you're done.â€\n\nThat was just the forward pass.\n\nThen came backpropagation. And holy hell â€” the math hit hard. Thatâ€™s where the real magic lies. It's the silent engine powering every model youâ€™ve ever used, and it nearly broke me at first.\n\nOnce I wrapped my head around it, implementing layers was easy â€” just more neurons bundled together, abstracted into units. But then I stumbled upon batching. I had no clue what it wasâ€¦ until my simple model took 10 minutes on basic datasets. Turns out batching wasnâ€™t optional â€” it was critical.\n\nThis was just the beginning!"
        },
        {
          "type": "heading",
          "content": "Sucess ! or so I thought , Mnist came back with 95% accuracy ğŸ¤¯ "
        },
        {
          "type": "paragraph",
          "content": "I was like â€” wow, this is insane. How did I even pull that off? That dopamine hit was real. The feeling? Absolutely wild.\n\nI was chasing 100% accuracy like a madman. At that point, I had only implemented basic stuff: batching, DenseLayer, ReLU, and Softmax. But as I kept reading, I stumbled on CNNs â€” and every post said they give way better accuracy.\n\nSo, high on that dopamine rush, I dove headfirst into convolutional layers.\n\nDebugging the math behind kernels was brutal. I had a rough idea of how they worked, but implementing them was a whole different beast. I took a peek at some open-source examples online and pieced things together. But it got crazy real fast â€” I ended up with a 5-deep for-loop ğŸ’€, and unsurprisingly, training time exploded to 5+ hours on simple data.\n\nThatâ€™s when I knew â€” I had to vectorize the operations. I tried with NumPy... and yeah, that didnâ€™t go as planned either ğŸ˜µâ€ğŸ’«."
        },
        {
          "type": "heading",
          "content": "Roadblock !? how to stand on the shoulders of gaints"
        },
        {
          "type": "paragraph",
          "content": "I was like â€” how the hell are these guys running convolutions so fast in TensorFlow?!\n\nMy framework was taking 20 minutes per epoch on MNIST, and meanwhile, TF blazed through in 1 second. I thought my implementation was broken. Then I looked deeper.\n\nTurns out... these guys are CHEATERS ğŸ˜¤\nThey offload everything to C/C++ under the hood â€” kernels, matrix ops, convolutions, all optimized to the max.\n\nI couldnâ€™t match that firepower directly, so yeah... I â€œcheatedâ€ too.\n\nI plugged in PyTorch convolutions to speed things up, but to be fair â€” I still implemented other low-level stuff myself, like im2col and col2im, to optimize my pipeline.\n\nAnd guess what?\n\nAfter all that madness â€” I hit 98% accuracy on MNIST. ğŸ¤¯\nThat moment was pure validation. It felt like standing on the shoulders of giantsâ€¦ while wearing my own hacked-together shoes."
        },
        {
          "type": "heading",
          "content": "So I implemented this , wheres the result moment hit me ?"
        },
        {
          "type": "paragraph",
          "content": "I was hyped. I had just built a model that hit 98% accuracy on MNIST. Naturally, I thought: \"Letâ€™s see this beast in action!\"\n\nSo I quickly whipped up a simple Tkinter interface to test it with my own handwriting. I was expecting the model to easily get 9 out of 10 digits right.\n\nTurns out... I was completely wrong.\n\nThe model choked. It performed terribly.\n\nThatâ€™s when it hit me â€” this was one of those moments. High accuracy on clean test data doesnâ€™t always mean good real-world performance.\n\nSo I dove into production-related ML concepts and found out about data augmentation â€” subtly modifying inputs to make the model more robust and avoid overfitting.\n\nImplementing some of it was chill â€” flips, shifts, rotations. But zooming? That one gave me a headache ğŸ’€\n\nStill, it worked like a charm. After retraining with augmented data, my accuracy dropped slightly to 96.9%, but the model started generalizing way better.\n\nAnd finally, yes â€” it got 9/10 digits correct.\n\nğŸ¥³ Redemption.\n"
        },
        {
          "type": "heading",
          "content": "Next Steps ? language processing âœ”ï¸"
        },
        {
          "type": "paragraph",
          "content": "After a while, I got bored of image stuff â€” MNIST, EMNIST... meh. So I thought, â€œWhy not expand to a new domain?â€\n\nThatâ€™s when I jumped into NLP. I built a quick TextManager class that used TfidfVectorizer to convert text into numbers. It was simple enough.\n\nSo I grabbed a dataset from Kaggle[https://www.kaggle.com/search], aiming to classify whether a piece of text contained hate speech or not. Easy, right? I already had Softmax, CrossEntropy, and the works from my earlier models.\n\nThen came the chaos.\n\nI tested it â€” and no matter what I typed, the model kept saying it was hate speech ğŸ˜\n\nI was confused. Like... what is this model on?!\n\nEventually, I asked the real MVP â€” ChatGPT â€” and it hit me with a new word: Class Imbalance. Turns out my dataset had an 85:15 ratio, with 85% of entries labeled as hate speech.\n\nThe dataset was scraped from Twitter.\nWhat did I expect ğŸ˜”\n\nI learned that in such cases, your model can easily learn to just say â€œhateâ€ for everything and still get high accuracy. I tried using class weights, tweaking loss functions â€” but even then, I couldnâ€™t push accuracy beyond 20% on non-hate examples.\n\nThatâ€™s when I realized... NLP isnâ€™t just image classification with words.\nThereâ€™s too much math. Too much chaos.\n\nAnd with that, I temporarily retired my NLP dreams. ğŸª¦"
        },
        {
          "type": "heading",
          "content": "Idk anymore Focussed on other projects for a while\n"
        },
        {
          "type": "paragraph",
          "content": "After a break, I came full circle â€” back to working on GDNet. But after a rough loss, I wasnâ€™t sure what to do next.\n\nThatâ€™s when I remembered a side project I had left hanging: weather prediction.\n\nSo I jumped into time series analysis. Learned about RNNs and LSTMs â€” donâ€™t mind my tech jargon ğŸ˜. It was honestly a fun way to get back into things. I wrapped up that project, feeling good...\n\n...until YouTube betrayed me.\n\nRight after finishing the LSTM model, I got hit with:\n\nâ€œWhy RNNs Are Bad?â€\n\nâ€œWhat Replaced LSTMs?â€\n\nARE YOU KIDDING ME, YouTube? Why didnâ€™t you show me this before?! ğŸ˜­\n\nAnyway, I watched those, and thatâ€™s when I got introduced to attention layers â€” and of course, \"Attention is All You Need\".\n\nThat paper? ğŸ”¥ Wild. I dove into it, and with some help from online resources, I started building my own Transformer blocks, including things like MultiHeadAttentionLayer, feedforward layers, layer norms, the whole setup.\n\nI was excited again. Maybe... just maybe... I could build my own mini GPT at home.\n\nI already knew the kind of compute power required for LLMs, but that didnâ€™t stop me. I trained a tiny version:\n\n~100k tokens\n\n~30k vocab\n\n~10 epochs\n\nAccuracy: ~30%\n\nEstimated time to train a â€œrealâ€ one: 67 days ğŸ’€\n\nTo squeeze out more performance, I even started looking into optimization techniques like matrix decomposition â€” especially for the value matrix in attention â€” to reduce computational overhead. Helped a bit, but not miracle-level.\n\nFeeling confident, I tried showing off how modular my framework had become by re-implementing my CNN model... but guess what?\n\nIt completely broke.\n\nTurns out my â€œframeworkâ€ was just a single 1.1k-line Python file stitched together with dreams and duct tape ğŸ˜‚\n\nThe End!?\nMaybe. Maybe not. ğŸ’€"
        },
        {
          "type": "heading",
          "content": "I sat and modularized my modules for the next 5 hours "
        },
        {
          "type": "paragraph",
          "content": "The pain of modularizing my framework was slightly spared since the codebase was just 1.1k lines â€” but donâ€™t get me wrong, it was still painful.\n\nI sat down and refactored the whole thing, determined never to face this kind of mess again. And in the process, something clicked ğŸ•µï¸: Iâ€™d been tunnel-visioned on just the layers.\n\nTurns out, thereâ€™s a whole world beyond just Dense, ReLU, and Conv2D. I had totally sidelined the importance of:\n\nOptimizers (SGD isnâ€™t everything, shocker)\n\nDiverse activation functions\n\nAdvanced loss functions\n\nRegularization techniques\n\nSo I hit supersonic mode and started exploring them. It was honestly fun as hell. My framework finally started to feel like a real deep learning library.\n\nIâ€™m still building and improving it â€” and if youâ€™re someone interested in learning how DL frameworks work under the hood, youâ€™re welcome to join in!\n\nThe code might be a little messy (I meanâ€¦ itâ€™s handcrafted with love ğŸ˜…), but everything is readable, and most of it can be understood with a quick web search.\nLink to my REPO - gdnet [https://github.com/Ghua8088/GDnet ] check it out :)"
        },
        {
          "type": "heading",
          "content": "Main Villains :"
        },
        {
          "type": "list",
          "content": "ğŸŸ¥ ShapeMismatchError\nğŸ” CuPy vs NumPy mismatch\nğŸ“¡ Broadcasting errors\nğŸ˜µâ€ğŸ’« Random NaNs out of nowhere\nğŸ‘¿ Derivative of Softmax\n"
        }
      ]
    }
  }
]