[
  {
    "id": "first-post",
    "title": "How I Built This Site",
    "date": "2025-06-23",
    "description": "A breakdown of how I built this project using React and Bootstrap.",
    "thumbnail": "helloworld.png",
    "content": {
      "sections": [
        {
          "type": "paragraph",
          "content": "Building a personal website is always an exciting journey. In this post, I'll walk you through how I created this blog using React and modern web technologies."
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Tech Stack"
        },
        {
          "type": "paragraph",
          "content": "For this project, I chose React as the frontend framework due to its component-based architecture and excellent developer experience. Here's what I used:"
        },
        {
          "type": "list",
          "items": [
            "React 18 for the UI framework",
            "React Router for navigation",
            "Bootstrap for responsive design",
            "CSS3 for custom styling",
            "JSON for data management"
          ]
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Architecture"
        },
        {
          "type": "paragraph",
          "content": "The site follows a clean, component-based architecture where each page is a separate component. Instead of using markdown files, I opted for a JSON-based content management system that provides better performance and easier maintenance."
        },
        {
          "type": "code",
          "language": "javascript",
          "content": "// Example component structure\nfunction BlogPost() {\n  const [blogData, setBlogData] = useState(null);\n  \n  useEffect(() => {\n    // Load blog content from JSON\n    const blog = blogData.find(b => b.id === id);\n    setBlogData(blog);\n  }, [id]);\n  \n  return (\n    <div className=\"blog-content\">\n      <h1>{blogData.title}</h1>\n      {blogData.content.sections.map(section => (\n        <RenderSection key={section.id} section={section} />\n      ))}\n    </div>\n  );\n}"
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Design Philosophy"
        },
        {
          "type": "paragraph",
          "content": "The design focuses on a clean, matte black aesthetic with subtle gradients and smooth animations. The color scheme emphasizes readability while maintaining a modern, professional look."
        },
        {
          "type": "paragraph",
          "content": "Key design principles include:"
        },
        {
          "type": "list",
          "items": [
            "High contrast for accessibility",
            "Smooth transitions and hover effects",
            "Responsive design for all devices",
            "Clean typography with Inter font",
            "Subtle shadows and depth"
          ]
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Performance Optimizations"
        },
        {
          "type": "paragraph",
          "content": "To ensure fast loading times, I implemented several optimizations:"
        },
        {
          "type": "list",
          "items": [
            "Lazy loading for images",
            "Component-based code splitting",
            "Optimized CSS with minimal reflows",
            "Efficient state management",
            "JSON-based content loading"
          ]
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Future Enhancements"
        },
        {
          "type": "paragraph",
          "content": "The site is designed to be easily extensible. Some planned features include:"
        },
        {
          "type": "list",
          "items": [
            "Comment system",
            "Social media integration",
            "Analytics dashboard"
          ]
        },
        {
          "type": "paragraph",
          "content": "Building this website has been a great learning experience. The combination of React's flexibility and modern CSS techniques has resulted in a fast, beautiful, and maintainable codebase."
        }
      ]
    }
  },
  {
    "id": "intro-to-react",
    "title": "Intro to React",
    "date": "2025-06-20",
    "description": "Everything you need to know to start React from scratch.",
    "thumbnail": "default.png",
    "content": {
      "sections": [
        {
          "type": "paragraph",
          "content": "React is a powerful JavaScript library for building user interfaces. In this comprehensive guide, I'll walk you through the fundamentals of React development."
        },
        {
          "type": "heading",
          "level": 2,
          "content": "What is React?"
        },
        {
          "type": "paragraph",
          "content": "React is a declarative, efficient, and flexible JavaScript library for building user interfaces. It lets you compose complex UIs from small and isolated pieces of code called 'components'."
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Key Concepts"
        },
        {
          "type": "list",
          "items": [
            "Components: Reusable UI pieces",
            "Props: Data passed to components",
            "State: Internal component data",
            "JSX: JavaScript syntax extension",
            "Virtual DOM: Efficient rendering"
          ]
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Getting Started"
        },
        {
          "type": "code",
          "language": "bash",
          "content": "npx create-react-app my-app\ncd my-app\nnpm start"
        },
        {
          "type": "paragraph",
          "content": "This will create a new React project with all the necessary dependencies and start the development server."
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Your First Component"
        },
        {
          "type": "code",
          "language": "jsx",
          "content": "function Welcome(props) {\n  return <h1>Hello, {props.name}!</h1>;\n}\n\nfunction App() {\n  return (\n    <div>\n      <Welcome name=\"React Developer\" />\n    </div>\n  );\n}"
        },
        {
          "type": "paragraph",
          "content": "This simple example shows how components work in React. The Welcome component receives props and renders them in JSX."
        },
        {
          "type": "heading",
          "level": 2,
          "content": "State Management"
        },
        {
          "type": "paragraph",
          "content": "State allows components to manage their own data and re-render when that data changes."
        },
        {
          "type": "code",
          "language": "jsx",
          "content": "import { useState } from 'react';\n\nfunction Counter() {\n  const [count, setCount] = useState(0);\n  \n  return (\n    <div>\n      <p>Count: {count}</p>\n      <button onClick={() => setCount(count + 1)}>\n        Increment\n      </button>\n    </div>\n  );\n}"
        },
        {
          "type": "heading",
          "level": 2,
          "content": "Best Practices"
        },
        {
          "type": "list",
          "items": [
            "Keep components small and focused",
            "Use meaningful component names",
            "Extract reusable logic into custom hooks",
            "Optimize performance with React.memo",
            "Follow the single responsibility principle"
          ]
        },
        {
          "type": "paragraph",
          "content": "React is an excellent choice for building modern web applications. Its component-based architecture and rich ecosystem make it a powerful tool for developers."
        }
      ]
    }
  },
  {
    "id": "gdnet-project-overview",
    "title": "GDnet Project Overview",
    "date": "2025-06-28",
    "description": "How i made my minimalistic educational  Deep Learning framework using cupy for GPU acceleration",
    "thumbnail": "deeplearning.png",
    "content": {
      "sections": [
        {
          "type": "heading",
          "content": "Kick off"
        },
        {
          "type": "paragraph",
          "content": "I always wanted to understand how deep learning frameworks like TensorFlow or PyTorch really work under the hood. So I dived into the math and quickly whipped up a basic NumPy-based neural net. At first, I thought, ‚ÄúThis is a piece of cake ‚Äî just W¬∑X + B and boom, you're done.‚Äù\n\nThat was just the forward pass.\n\nThen came backpropagation. And holy hell ‚Äî the math hit hard. That‚Äôs where the real magic lies. It's the silent engine powering every model you‚Äôve ever used, and it nearly broke me at first.\n\nOnce I wrapped my head around it, implementing layers was easy ‚Äî just more neurons bundled together, abstracted into units. But then I stumbled upon batching. I had no clue what it was‚Ä¶ until my simple model took 10 minutes on basic datasets. Turns out batching wasn‚Äôt optional ‚Äî it was critical.\n\nThis was just the beginning!"
        },
        {
          "type": "heading",
          "content": "Sucess ! or so I thought , Mnist came back with 95% accuracy ü§Ø "
        },
        {
          "type": "paragraph",
          "content": "I was like ‚Äî wow, this is insane. How did I even pull that off? That dopamine hit was real. The feeling? Absolutely wild.\n\nI was chasing 100% accuracy like a madman. At that point, I had only implemented basic stuff: batching, DenseLayer, ReLU, and Softmax. But as I kept reading, I stumbled on CNNs ‚Äî and every post said they give way better accuracy.\n\nSo, high on that dopamine rush, I dove headfirst into convolutional layers.\n\nDebugging the math behind kernels was brutal. I had a rough idea of how they worked, but implementing them was a whole different beast. I took a peek at some open-source examples online and pieced things together. But it got crazy real fast ‚Äî I ended up with a 5-deep for-loop üíÄ, and unsurprisingly, training time exploded to 5+ hours on simple data.\n\nThat‚Äôs when I knew ‚Äî I had to vectorize the operations. I tried with NumPy... and yeah, that didn‚Äôt go as planned either üòµ‚Äçüí´."
        },
        {
          "type": "heading",
          "content": "Roadblock !? how to stand on the shoulders of gaints"
        },
        {
          "type": "paragraph",
          "content": "I was like ‚Äî how the hell are these guys running convolutions so fast in TensorFlow?!\n\nMy framework was taking 20 minutes per epoch on MNIST, and meanwhile, TF blazed through in 1 second. I thought my implementation was broken. Then I looked deeper.\n\nTurns out... these guys are CHEATERS üò§\nThey offload everything to C/C++ under the hood ‚Äî kernels, matrix ops, convolutions, all optimized to the max.\n\nI couldn‚Äôt match that firepower directly, so yeah... I ‚Äúcheated‚Äù too.\n\nI plugged in PyTorch convolutions to speed things up, but to be fair ‚Äî I still implemented other low-level stuff myself, like im2col and col2im, to optimize my pipeline.\n\nAnd guess what?\n\nAfter all that madness ‚Äî I hit 98% accuracy on MNIST. ü§Ø\nThat moment was pure validation. It felt like standing on the shoulders of giants‚Ä¶ while wearing my own hacked-together shoes."
        },
        {
          "type": "heading",
          "content": "So I implemented this , wheres the result moment hit me ?"
        },
        {
          "type": "paragraph",
          "content": "I was hyped. I had just built a model that hit 98% accuracy on MNIST. Naturally, I thought: \"Let‚Äôs see this beast in action!\"\n\nSo I quickly whipped up a simple Tkinter interface to test it with my own handwriting. I was expecting the model to easily get 9 out of 10 digits right.\n\nTurns out... I was completely wrong.\n\nThe model choked. It performed terribly.\n\nThat‚Äôs when it hit me ‚Äî this was one of those moments. High accuracy on clean test data doesn‚Äôt always mean good real-world performance.\n\nSo I dove into production-related ML concepts and found out about data augmentation ‚Äî subtly modifying inputs to make the model more robust and avoid overfitting.\n\nImplementing some of it was chill ‚Äî flips, shifts, rotations. But zooming? That one gave me a headache üíÄ\n\nStill, it worked like a charm. After retraining with augmented data, my accuracy dropped slightly to 96.9%, but the model started generalizing way better.\n\nAnd finally, yes ‚Äî it got 9/10 digits correct.\n\nü•≥ Redemption.\n"
        },
        {
          "type": "heading",
          "content": "Next Steps ? language processing ‚úîÔ∏è"
        },
        {
          "type": "paragraph",
          "content": "After a while, I got bored of image stuff ‚Äî MNIST, EMNIST... meh. So I thought, ‚ÄúWhy not expand to a new domain?‚Äù\n\nThat‚Äôs when I jumped into NLP. I built a quick TextManager class that used TfidfVectorizer to convert text into numbers. It was simple enough.\n\nSo I grabbed a dataset from Kaggle[https://www.kaggle.com/search], aiming to classify whether a piece of text contained hate speech or not. Easy, right? I already had Softmax, CrossEntropy, and the works from my earlier models.\n\nThen came the chaos.\n\nI tested it ‚Äî and no matter what I typed, the model kept saying it was hate speech üòê\n\nI was confused. Like... what is this model on?!\n\nEventually, I asked the real MVP ‚Äî ChatGPT ‚Äî and it hit me with a new word: Class Imbalance. Turns out my dataset had an 85:15 ratio, with 85% of entries labeled as hate speech.\n\nThe dataset was scraped from Twitter.\nWhat did I expect üòî\n\nI learned that in such cases, your model can easily learn to just say ‚Äúhate‚Äù for everything and still get high accuracy. I tried using class weights, tweaking loss functions ‚Äî but even then, I couldn‚Äôt push accuracy beyond 20% on non-hate examples.\n\nThat‚Äôs when I realized... NLP isn‚Äôt just image classification with words.\nThere‚Äôs too much math. Too much chaos.\n\nAnd with that, I temporarily retired my NLP dreams. ü™¶"
        },
        {
          "type": "heading",
          "content": "Idk anymore Focussed on other projects for a while\n"
        },
        {
          "type": "paragraph",
          "content": "After a break, I came full circle ‚Äî back to working on GDNet. But after a rough loss, I wasn‚Äôt sure what to do next.\n\nThat‚Äôs when I remembered a side project I had left hanging: weather prediction.\n\nSo I jumped into time series analysis. Learned about RNNs and LSTMs ‚Äî don‚Äôt mind my tech jargon üòé. It was honestly a fun way to get back into things. I wrapped up that project, feeling good...\n\n...until YouTube betrayed me.\n\nRight after finishing the LSTM model, I got hit with:\n\n‚ÄúWhy RNNs Are Bad?‚Äù\n\n‚ÄúWhat Replaced LSTMs?‚Äù\n\nARE YOU KIDDING ME, YouTube? Why didn‚Äôt you show me this before?! üò≠\n\nAnyway, I watched those, and that‚Äôs when I got introduced to attention layers ‚Äî and of course, \"Attention is All You Need\".\n\nThat paper? üî• Wild. I dove into it, and with some help from online resources, I started building my own Transformer blocks, including things like MultiHeadAttentionLayer, feedforward layers, layer norms, the whole setup.\n\nI was excited again. Maybe... just maybe... I could build my own mini GPT at home.\n\nI already knew the kind of compute power required for LLMs, but that didn‚Äôt stop me. I trained a tiny version:\n\n~100k tokens\n\n~30k vocab\n\n~10 epochs\n\nAccuracy: ~30%\n\nEstimated time to train a ‚Äúreal‚Äù one: 67 days üíÄ\n\nTo squeeze out more performance, I even started looking into optimization techniques like matrix decomposition ‚Äî especially for the value matrix in attention ‚Äî to reduce computational overhead. Helped a bit, but not miracle-level.\n\nFeeling confident, I tried showing off how modular my framework had become by re-implementing my CNN model... but guess what?\n\nIt completely broke.\n\nTurns out my ‚Äúframework‚Äù was just a single 1.1k-line Python file stitched together with dreams and duct tape üòÇ\n\nThe End!?\nMaybe. Maybe not. üíÄ"
        },
        {
          "type": "heading",
          "content": "I sat and modularized my modules for the next 5 hours "
        },
        {
          "type": "paragraph",
          "content": "The pain of modularizing my framework was slightly spared since the codebase was just 1.1k lines ‚Äî but don‚Äôt get me wrong, it was still painful.\n\nI sat down and refactored the whole thing, determined never to face this kind of mess again. And in the process, something clicked üïµÔ∏è: I‚Äôd been tunnel-visioned on just the layers.\n\nTurns out, there‚Äôs a whole world beyond just Dense, ReLU, and Conv2D. I had totally sidelined the importance of:\n\nOptimizers (SGD isn‚Äôt everything, shocker)\n\nDiverse activation functions\n\nAdvanced loss functions\n\nRegularization techniques\n\nSo I hit supersonic mode and started exploring them. It was honestly fun as hell. My framework finally started to feel like a real deep learning library.\n\nI‚Äôm still building and improving it ‚Äî and if you‚Äôre someone interested in learning how DL frameworks work under the hood, you‚Äôre welcome to join in!\n\nThe code might be a little messy (I mean‚Ä¶ it‚Äôs handcrafted with love üòÖ), but everything is readable, and most of it can be understood with a quick web search.\nLink to my REPO - gdnet [https://github.com/Ghua8088/GDnet ] check it out :)"
        },
        {
          "type": "heading",
          "content": "Main Villains :"
        },
        {
          "type": "list",
          "content": "üü• ShapeMismatchError\nüîÅ CuPy vs NumPy mismatch\nüì° Broadcasting errors\nüòµ‚Äçüí´ Random NaNs out of nowhere\nüëø Derivative of Softmax\n"
        }
      ]
    }
  }
]